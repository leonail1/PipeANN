// https://github.com/FALCONN-LIB/FFHT

// The MIT License (MIT)

// Copyright (c) 2015 Alexandr Andoni, Piotr Indyk, Thijs Laarhoven,
// Ilya Razenshteyn, Ludwig Schmidt

// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:

// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.

// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
// THE SOFTWARE.

#pragma once

inline void helper_float_1(float *buf);
inline void helper_float_1(float *buf) {
  for (int j = 0; j < 2; j += 2) {
    for (int k = 0; k < 1; ++k) {
      float u = buf[j + k];
      float v = buf[j + k + 1];
      buf[j + k] = u + v;
      buf[j + k + 1] = u - v;
    }
  }
}
inline void helper_float_2(float *buf);
inline void helper_float_2(float *buf) {
  for (int j = 0; j < 4; j += 2) {
    for (int k = 0; k < 1; ++k) {
      float u = buf[j + k];
      float v = buf[j + k + 1];
      buf[j + k] = u + v;
      buf[j + k + 1] = u - v;
    }
  }
  for (int j = 0; j < 4; j += 4) {
    for (int k = 0; k < 2; ++k) {
      float u = buf[j + k];
      float v = buf[j + k + 2];
      buf[j + k] = u + v;
      buf[j + k + 2] = u - v;
    }
  }
}
inline void helper_float_3(float *buf);
inline void helper_float_3(float *buf) {
  for (int j = 0; j < 8; j += 8) {
    __asm__ volatile(
        "vmovups (%0), %%ymm0\n"
        "vpermilps $160, %%ymm0, %%ymm8\n"
        "vpermilps $245, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
        "vpermilps $68, %%ymm0, %%ymm8\n"
        "vpermilps $238, %%ymm0, %%ymm9\n"
        "vxorps %%ymm10, %%ymm10, %%ymm10\n"
        "vsubps %%ymm9, %%ymm10, %%ymm11\n"
        "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
        "vaddps %%ymm8, %%ymm12, %%ymm0\n"
        "vxorps %%ymm8, %%ymm8, %%ymm8\n"
        "vsubps %%ymm0, %%ymm8, %%ymm9\n"
        "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
        "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
        "vaddps %%ymm10, %%ymm11, %%ymm0\n"
        "vmovups %%ymm0, (%0)\n" ::"r"(buf + j)
        : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10", "%ymm11",
          "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
  }
}
inline void helper_float_4(float *buf);
inline void helper_float_4(float *buf) {
  for (int j = 0; j < 16; j += 16) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
            "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
}
inline void helper_float_5(float *buf);
inline void helper_float_5(float *buf) {
  for (int j = 0; j < 32; j += 32) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vmovups %%ymm0, (%0)\n"
          "vmovups %%ymm1, (%1)\n"
          "vmovups %%ymm2, (%2)\n"
          "vmovups %%ymm3, (%3)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
            "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
}
inline void helper_float_6(float *buf);
inline void helper_float_6(float *buf) {
  for (int j = 0; j < 64; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
          "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
            "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
}
inline void helper_float_7_recursive(float *buf, int depth);
inline void helper_float_7_recursive(float *buf, int depth) {
  if (depth == 7) {
    for (int j = 0; j < 128; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vmovups (%4), %%ymm4\n"
            "vmovups (%5), %%ymm5\n"
            "vmovups (%6), %%ymm6\n"
            "vmovups (%7), %%ymm7\n"
            "vpermilps $160, %%ymm0, %%ymm8\n"
            "vpermilps $245, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
            "vpermilps $160, %%ymm1, %%ymm8\n"
            "vpermilps $245, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
            "vpermilps $160, %%ymm2, %%ymm8\n"
            "vpermilps $245, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
            "vpermilps $160, %%ymm3, %%ymm8\n"
            "vpermilps $245, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
            "vpermilps $160, %%ymm4, %%ymm8\n"
            "vpermilps $245, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
            "vpermilps $160, %%ymm5, %%ymm8\n"
            "vpermilps $245, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
            "vpermilps $160, %%ymm6, %%ymm8\n"
            "vpermilps $245, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
            "vpermilps $160, %%ymm7, %%ymm8\n"
            "vpermilps $245, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
            "vpermilps $68, %%ymm0, %%ymm8\n"
            "vpermilps $238, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm0\n"
            "vpermilps $68, %%ymm1, %%ymm8\n"
            "vpermilps $238, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm1\n"
            "vpermilps $68, %%ymm2, %%ymm8\n"
            "vpermilps $238, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm2\n"
            "vpermilps $68, %%ymm3, %%ymm8\n"
            "vpermilps $238, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm3\n"
            "vpermilps $68, %%ymm4, %%ymm8\n"
            "vpermilps $238, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm4\n"
            "vpermilps $68, %%ymm5, %%ymm8\n"
            "vpermilps $238, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm5\n"
            "vpermilps $68, %%ymm6, %%ymm8\n"
            "vpermilps $238, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm6\n"
            "vpermilps $68, %%ymm7, %%ymm8\n"
            "vpermilps $238, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm7\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm0, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm0\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm1, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm1\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm2, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm2\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm3, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm3\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm4, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm4\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm5, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm5\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm6, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm6\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm7, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm7\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm5, %%ymm4, %%ymm12\n"
            "vsubps %%ymm5, %%ymm4, %%ymm13\n"
            "vaddps %%ymm7, %%ymm6, %%ymm14\n"
            "vsubps %%ymm7, %%ymm6, %%ymm15\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vaddps %%ymm14, %%ymm12, %%ymm4\n"
            "vsubps %%ymm14, %%ymm12, %%ymm6\n"
            "vaddps %%ymm15, %%ymm13, %%ymm5\n"
            "vsubps %%ymm15, %%ymm13, %%ymm7\n"
            "vaddps %%ymm4, %%ymm0, %%ymm8\n"
            "vsubps %%ymm4, %%ymm0, %%ymm12\n"
            "vaddps %%ymm5, %%ymm1, %%ymm9\n"
            "vsubps %%ymm5, %%ymm1, %%ymm13\n"
            "vaddps %%ymm6, %%ymm2, %%ymm10\n"
            "vsubps %%ymm6, %%ymm2, %%ymm14\n"
            "vaddps %%ymm7, %%ymm3, %%ymm11\n"
            "vsubps %%ymm7, %%ymm3, %%ymm15\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n"
            "vmovups %%ymm10, (%2)\n"
            "vmovups %%ymm11, (%3)\n"
            "vmovups %%ymm12, (%4)\n"
            "vmovups %%ymm13, (%5)\n"
            "vmovups %%ymm14, (%6)\n"
            "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
            "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    for (int j = 0; j < 128; j += 128) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 64)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    return;
  }
}
inline void helper_float_7(float *buf);
inline void helper_float_7(float *buf) {
  helper_float_7_recursive(buf, 7);
}
inline void helper_float_8_recursive(float *buf, int depth);
inline void helper_float_8_recursive(float *buf, int depth) {
  if (depth == 6) {
    for (int j = 0; j < 64; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vmovups (%4), %%ymm4\n"
            "vmovups (%5), %%ymm5\n"
            "vmovups (%6), %%ymm6\n"
            "vmovups (%7), %%ymm7\n"
            "vpermilps $160, %%ymm0, %%ymm8\n"
            "vpermilps $245, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
            "vpermilps $160, %%ymm1, %%ymm8\n"
            "vpermilps $245, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
            "vpermilps $160, %%ymm2, %%ymm8\n"
            "vpermilps $245, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
            "vpermilps $160, %%ymm3, %%ymm8\n"
            "vpermilps $245, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
            "vpermilps $160, %%ymm4, %%ymm8\n"
            "vpermilps $245, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
            "vpermilps $160, %%ymm5, %%ymm8\n"
            "vpermilps $245, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
            "vpermilps $160, %%ymm6, %%ymm8\n"
            "vpermilps $245, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
            "vpermilps $160, %%ymm7, %%ymm8\n"
            "vpermilps $245, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
            "vpermilps $68, %%ymm0, %%ymm8\n"
            "vpermilps $238, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm0\n"
            "vpermilps $68, %%ymm1, %%ymm8\n"
            "vpermilps $238, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm1\n"
            "vpermilps $68, %%ymm2, %%ymm8\n"
            "vpermilps $238, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm2\n"
            "vpermilps $68, %%ymm3, %%ymm8\n"
            "vpermilps $238, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm3\n"
            "vpermilps $68, %%ymm4, %%ymm8\n"
            "vpermilps $238, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm4\n"
            "vpermilps $68, %%ymm5, %%ymm8\n"
            "vpermilps $238, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm5\n"
            "vpermilps $68, %%ymm6, %%ymm8\n"
            "vpermilps $238, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm6\n"
            "vpermilps $68, %%ymm7, %%ymm8\n"
            "vpermilps $238, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm7\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm0, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm0\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm1, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm1\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm2, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm2\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm3, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm3\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm4, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm4\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm5, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm5\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm6, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm6\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm7, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm7\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm5, %%ymm4, %%ymm12\n"
            "vsubps %%ymm5, %%ymm4, %%ymm13\n"
            "vaddps %%ymm7, %%ymm6, %%ymm14\n"
            "vsubps %%ymm7, %%ymm6, %%ymm15\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vaddps %%ymm14, %%ymm12, %%ymm4\n"
            "vsubps %%ymm14, %%ymm12, %%ymm6\n"
            "vaddps %%ymm15, %%ymm13, %%ymm5\n"
            "vsubps %%ymm15, %%ymm13, %%ymm7\n"
            "vaddps %%ymm4, %%ymm0, %%ymm8\n"
            "vsubps %%ymm4, %%ymm0, %%ymm12\n"
            "vaddps %%ymm5, %%ymm1, %%ymm9\n"
            "vsubps %%ymm5, %%ymm1, %%ymm13\n"
            "vaddps %%ymm6, %%ymm2, %%ymm10\n"
            "vsubps %%ymm6, %%ymm2, %%ymm14\n"
            "vaddps %%ymm7, %%ymm3, %%ymm11\n"
            "vsubps %%ymm7, %%ymm3, %%ymm15\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n"
            "vmovups %%ymm10, (%2)\n"
            "vmovups %%ymm11, (%3)\n"
            "vmovups %%ymm12, (%4)\n"
            "vmovups %%ymm13, (%5)\n"
            "vmovups %%ymm14, (%6)\n"
            "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
            "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    return;
  }
  if (depth == 8) {
    helper_float_8_recursive(buf + 0, 6);
    helper_float_8_recursive(buf + 64, 6);
    helper_float_8_recursive(buf + 128, 6);
    helper_float_8_recursive(buf + 192, 6);
    for (int j = 0; j < 256; j += 256) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vmovups %%ymm0, (%0)\n"
            "vmovups %%ymm1, (%1)\n"
            "vmovups %%ymm2, (%2)\n"
            "vmovups %%ymm3, (%3)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    return;
  }
}
inline void helper_float_8(float *buf);
inline void helper_float_8(float *buf) {
  helper_float_8_recursive(buf, 8);
}
inline void helper_float_9(float *buf);
inline void helper_float_9(float *buf) {
  for (int j = 0; j < 512; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
          "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
            "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 512; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256),
          "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
            "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
}
inline void helper_float_10_recursive(float *buf, int depth);
inline void helper_float_10_recursive(float *buf, int depth) {
  if (depth == 10) {
    for (int j = 0; j < 1024; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vmovups (%4), %%ymm4\n"
            "vmovups (%5), %%ymm5\n"
            "vmovups (%6), %%ymm6\n"
            "vmovups (%7), %%ymm7\n"
            "vpermilps $160, %%ymm0, %%ymm8\n"
            "vpermilps $245, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
            "vpermilps $160, %%ymm1, %%ymm8\n"
            "vpermilps $245, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
            "vpermilps $160, %%ymm2, %%ymm8\n"
            "vpermilps $245, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
            "vpermilps $160, %%ymm3, %%ymm8\n"
            "vpermilps $245, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
            "vpermilps $160, %%ymm4, %%ymm8\n"
            "vpermilps $245, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
            "vpermilps $160, %%ymm5, %%ymm8\n"
            "vpermilps $245, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
            "vpermilps $160, %%ymm6, %%ymm8\n"
            "vpermilps $245, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
            "vpermilps $160, %%ymm7, %%ymm8\n"
            "vpermilps $245, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
            "vpermilps $68, %%ymm0, %%ymm8\n"
            "vpermilps $238, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm0\n"
            "vpermilps $68, %%ymm1, %%ymm8\n"
            "vpermilps $238, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm1\n"
            "vpermilps $68, %%ymm2, %%ymm8\n"
            "vpermilps $238, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm2\n"
            "vpermilps $68, %%ymm3, %%ymm8\n"
            "vpermilps $238, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm3\n"
            "vpermilps $68, %%ymm4, %%ymm8\n"
            "vpermilps $238, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm4\n"
            "vpermilps $68, %%ymm5, %%ymm8\n"
            "vpermilps $238, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm5\n"
            "vpermilps $68, %%ymm6, %%ymm8\n"
            "vpermilps $238, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm6\n"
            "vpermilps $68, %%ymm7, %%ymm8\n"
            "vpermilps $238, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm7\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm0, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm0\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm1, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm1\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm2, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm2\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm3, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm3\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm4, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm4\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm5, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm5\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm6, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm6\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm7, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm7\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm5, %%ymm4, %%ymm12\n"
            "vsubps %%ymm5, %%ymm4, %%ymm13\n"
            "vaddps %%ymm7, %%ymm6, %%ymm14\n"
            "vsubps %%ymm7, %%ymm6, %%ymm15\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vaddps %%ymm14, %%ymm12, %%ymm4\n"
            "vsubps %%ymm14, %%ymm12, %%ymm6\n"
            "vaddps %%ymm15, %%ymm13, %%ymm5\n"
            "vsubps %%ymm15, %%ymm13, %%ymm7\n"
            "vaddps %%ymm4, %%ymm0, %%ymm8\n"
            "vsubps %%ymm4, %%ymm0, %%ymm12\n"
            "vaddps %%ymm5, %%ymm1, %%ymm9\n"
            "vsubps %%ymm5, %%ymm1, %%ymm13\n"
            "vaddps %%ymm6, %%ymm2, %%ymm10\n"
            "vsubps %%ymm6, %%ymm2, %%ymm14\n"
            "vaddps %%ymm7, %%ymm3, %%ymm11\n"
            "vsubps %%ymm7, %%ymm3, %%ymm15\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n"
            "vmovups %%ymm10, (%2)\n"
            "vmovups %%ymm11, (%3)\n"
            "vmovups %%ymm12, (%4)\n"
            "vmovups %%ymm13, (%5)\n"
            "vmovups %%ymm14, (%6)\n"
            "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
            "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    for (int j = 0; j < 1024; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vmovups (%4), %%ymm4\n"
            "vmovups (%5), %%ymm5\n"
            "vmovups (%6), %%ymm6\n"
            "vmovups (%7), %%ymm7\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm5, %%ymm4, %%ymm12\n"
            "vsubps %%ymm5, %%ymm4, %%ymm13\n"
            "vaddps %%ymm7, %%ymm6, %%ymm14\n"
            "vsubps %%ymm7, %%ymm6, %%ymm15\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vaddps %%ymm14, %%ymm12, %%ymm4\n"
            "vsubps %%ymm14, %%ymm12, %%ymm6\n"
            "vaddps %%ymm15, %%ymm13, %%ymm5\n"
            "vsubps %%ymm15, %%ymm13, %%ymm7\n"
            "vaddps %%ymm4, %%ymm0, %%ymm8\n"
            "vsubps %%ymm4, %%ymm0, %%ymm12\n"
            "vaddps %%ymm5, %%ymm1, %%ymm9\n"
            "vsubps %%ymm5, %%ymm1, %%ymm13\n"
            "vaddps %%ymm6, %%ymm2, %%ymm10\n"
            "vsubps %%ymm6, %%ymm2, %%ymm14\n"
            "vaddps %%ymm7, %%ymm3, %%ymm11\n"
            "vsubps %%ymm7, %%ymm3, %%ymm15\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n"
            "vmovups %%ymm10, (%2)\n"
            "vmovups %%ymm11, (%3)\n"
            "vmovups %%ymm12, (%4)\n"
            "vmovups %%ymm13, (%5)\n"
            "vmovups %%ymm14, (%6)\n"
            "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256),
            "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    for (int j = 0; j < 1024; j += 1024) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 512)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    return;
  }
}
inline void helper_float_10(float *buf);
inline void helper_float_10(float *buf) {
  helper_float_10_recursive(buf, 10);
}
inline void helper_float_11_recursive(float *buf, int depth);
inline void helper_float_11_recursive(float *buf, int depth) {
  if (depth == 11) {
    for (int j = 0; j < 2048; j += 64) {
      for (int k = 0; k < 8; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vmovups (%4), %%ymm4\n"
            "vmovups (%5), %%ymm5\n"
            "vmovups (%6), %%ymm6\n"
            "vmovups (%7), %%ymm7\n"
            "vpermilps $160, %%ymm0, %%ymm8\n"
            "vpermilps $245, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
            "vpermilps $160, %%ymm1, %%ymm8\n"
            "vpermilps $245, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
            "vpermilps $160, %%ymm2, %%ymm8\n"
            "vpermilps $245, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
            "vpermilps $160, %%ymm3, %%ymm8\n"
            "vpermilps $245, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
            "vpermilps $160, %%ymm4, %%ymm8\n"
            "vpermilps $245, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
            "vpermilps $160, %%ymm5, %%ymm8\n"
            "vpermilps $245, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
            "vpermilps $160, %%ymm6, %%ymm8\n"
            "vpermilps $245, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
            "vpermilps $160, %%ymm7, %%ymm8\n"
            "vpermilps $245, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
            "vpermilps $68, %%ymm0, %%ymm8\n"
            "vpermilps $238, %%ymm0, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm0\n"
            "vpermilps $68, %%ymm1, %%ymm8\n"
            "vpermilps $238, %%ymm1, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm1\n"
            "vpermilps $68, %%ymm2, %%ymm8\n"
            "vpermilps $238, %%ymm2, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm2\n"
            "vpermilps $68, %%ymm3, %%ymm8\n"
            "vpermilps $238, %%ymm3, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm3\n"
            "vpermilps $68, %%ymm4, %%ymm8\n"
            "vpermilps $238, %%ymm4, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm4\n"
            "vpermilps $68, %%ymm5, %%ymm8\n"
            "vpermilps $238, %%ymm5, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm5\n"
            "vpermilps $68, %%ymm6, %%ymm8\n"
            "vpermilps $238, %%ymm6, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm6\n"
            "vpermilps $68, %%ymm7, %%ymm8\n"
            "vpermilps $238, %%ymm7, %%ymm9\n"
            "vxorps %%ymm10, %%ymm10, %%ymm10\n"
            "vsubps %%ymm9, %%ymm10, %%ymm11\n"
            "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
            "vaddps %%ymm8, %%ymm12, %%ymm7\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm0, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm0\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm1, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm1\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm2, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm2\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm3, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm3\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm4, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm4\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm5, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm5\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm6, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm6\n"
            "vxorps %%ymm8, %%ymm8, %%ymm8\n"
            "vsubps %%ymm7, %%ymm8, %%ymm9\n"
            "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
            "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
            "vaddps %%ymm10, %%ymm11, %%ymm7\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm5, %%ymm4, %%ymm12\n"
            "vsubps %%ymm5, %%ymm4, %%ymm13\n"
            "vaddps %%ymm7, %%ymm6, %%ymm14\n"
            "vsubps %%ymm7, %%ymm6, %%ymm15\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vaddps %%ymm14, %%ymm12, %%ymm4\n"
            "vsubps %%ymm14, %%ymm12, %%ymm6\n"
            "vaddps %%ymm15, %%ymm13, %%ymm5\n"
            "vsubps %%ymm15, %%ymm13, %%ymm7\n"
            "vaddps %%ymm4, %%ymm0, %%ymm8\n"
            "vsubps %%ymm4, %%ymm0, %%ymm12\n"
            "vaddps %%ymm5, %%ymm1, %%ymm9\n"
            "vsubps %%ymm5, %%ymm1, %%ymm13\n"
            "vaddps %%ymm6, %%ymm2, %%ymm10\n"
            "vsubps %%ymm6, %%ymm2, %%ymm14\n"
            "vaddps %%ymm7, %%ymm3, %%ymm11\n"
            "vsubps %%ymm7, %%ymm3, %%ymm15\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n"
            "vmovups %%ymm10, (%2)\n"
            "vmovups %%ymm11, (%3)\n"
            "vmovups %%ymm12, (%4)\n"
            "vmovups %%ymm13, (%5)\n"
            "vmovups %%ymm14, (%6)\n"
            "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
            "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    for (int j = 0; j < 2048; j += 512) {
      for (int k = 0; k < 64; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vmovups (%4), %%ymm4\n"
            "vmovups (%5), %%ymm5\n"
            "vmovups (%6), %%ymm6\n"
            "vmovups (%7), %%ymm7\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm5, %%ymm4, %%ymm12\n"
            "vsubps %%ymm5, %%ymm4, %%ymm13\n"
            "vaddps %%ymm7, %%ymm6, %%ymm14\n"
            "vsubps %%ymm7, %%ymm6, %%ymm15\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vaddps %%ymm14, %%ymm12, %%ymm4\n"
            "vsubps %%ymm14, %%ymm12, %%ymm6\n"
            "vaddps %%ymm15, %%ymm13, %%ymm5\n"
            "vsubps %%ymm15, %%ymm13, %%ymm7\n"
            "vaddps %%ymm4, %%ymm0, %%ymm8\n"
            "vsubps %%ymm4, %%ymm0, %%ymm12\n"
            "vaddps %%ymm5, %%ymm1, %%ymm9\n"
            "vsubps %%ymm5, %%ymm1, %%ymm13\n"
            "vaddps %%ymm6, %%ymm2, %%ymm10\n"
            "vsubps %%ymm6, %%ymm2, %%ymm14\n"
            "vaddps %%ymm7, %%ymm3, %%ymm11\n"
            "vsubps %%ymm7, %%ymm3, %%ymm15\n"
            "vmovups %%ymm8, (%0)\n"
            "vmovups %%ymm9, (%1)\n"
            "vmovups %%ymm10, (%2)\n"
            "vmovups %%ymm11, (%3)\n"
            "vmovups %%ymm12, (%4)\n"
            "vmovups %%ymm13, (%5)\n"
            "vmovups %%ymm14, (%6)\n"
            "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256),
            "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    for (int j = 0; j < 2048; j += 2048) {
      for (int k = 0; k < 512; k += 8) {
        __asm__ volatile(
            "vmovups (%0), %%ymm0\n"
            "vmovups (%1), %%ymm1\n"
            "vmovups (%2), %%ymm2\n"
            "vmovups (%3), %%ymm3\n"
            "vaddps %%ymm1, %%ymm0, %%ymm8\n"
            "vsubps %%ymm1, %%ymm0, %%ymm9\n"
            "vaddps %%ymm3, %%ymm2, %%ymm10\n"
            "vsubps %%ymm3, %%ymm2, %%ymm11\n"
            "vaddps %%ymm10, %%ymm8, %%ymm0\n"
            "vsubps %%ymm10, %%ymm8, %%ymm2\n"
            "vaddps %%ymm11, %%ymm9, %%ymm1\n"
            "vsubps %%ymm11, %%ymm9, %%ymm3\n"
            "vmovups %%ymm0, (%0)\n"
            "vmovups %%ymm1, (%1)\n"
            "vmovups %%ymm2, (%2)\n"
            "vmovups %%ymm3, (%3)\n" ::"r"(buf + j + k + 0),
            "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536)
            : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
              "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
      }
    }
    return;
  }
}
inline void helper_float_11(float *buf);
inline void helper_float_11(float *buf) {
  helper_float_11_recursive(buf, 11);
}
inline void helper_float_12(float *buf);
inline void helper_float_12(float *buf) {
  for (int j = 0; j < 4096; j += 64) {
    for (int k = 0; k < 8; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vpermilps $160, %%ymm0, %%ymm8\n"
          "vpermilps $245, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm0\n"
          "vpermilps $160, %%ymm1, %%ymm8\n"
          "vpermilps $245, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm1\n"
          "vpermilps $160, %%ymm2, %%ymm8\n"
          "vpermilps $245, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm2\n"
          "vpermilps $160, %%ymm3, %%ymm8\n"
          "vpermilps $245, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm3\n"
          "vpermilps $160, %%ymm4, %%ymm8\n"
          "vpermilps $245, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm4\n"
          "vpermilps $160, %%ymm5, %%ymm8\n"
          "vpermilps $245, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm5\n"
          "vpermilps $160, %%ymm6, %%ymm8\n"
          "vpermilps $245, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm6\n"
          "vpermilps $160, %%ymm7, %%ymm8\n"
          "vpermilps $245, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vaddsubps %%ymm11, %%ymm8, %%ymm7\n"
          "vpermilps $68, %%ymm0, %%ymm8\n"
          "vpermilps $238, %%ymm0, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm0\n"
          "vpermilps $68, %%ymm1, %%ymm8\n"
          "vpermilps $238, %%ymm1, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm1\n"
          "vpermilps $68, %%ymm2, %%ymm8\n"
          "vpermilps $238, %%ymm2, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm2\n"
          "vpermilps $68, %%ymm3, %%ymm8\n"
          "vpermilps $238, %%ymm3, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm3\n"
          "vpermilps $68, %%ymm4, %%ymm8\n"
          "vpermilps $238, %%ymm4, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm4\n"
          "vpermilps $68, %%ymm5, %%ymm8\n"
          "vpermilps $238, %%ymm5, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm5\n"
          "vpermilps $68, %%ymm6, %%ymm8\n"
          "vpermilps $238, %%ymm6, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm6\n"
          "vpermilps $68, %%ymm7, %%ymm8\n"
          "vpermilps $238, %%ymm7, %%ymm9\n"
          "vxorps %%ymm10, %%ymm10, %%ymm10\n"
          "vsubps %%ymm9, %%ymm10, %%ymm11\n"
          "vblendps $204, %%ymm11, %%ymm9, %%ymm12\n"
          "vaddps %%ymm8, %%ymm12, %%ymm7\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm0, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm0, %%ymm0, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm0, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm0\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm1, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm1, %%ymm1, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm1, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm1\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm2, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm2, %%ymm2, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm2, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm2\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm3, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm3, %%ymm3, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm3, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm3\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm4, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm4, %%ymm4, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm4, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm4\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm5, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm5, %%ymm5, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm5, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm5\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm6, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm6, %%ymm6, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm6, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm6\n"
          "vxorps %%ymm8, %%ymm8, %%ymm8\n"
          "vsubps %%ymm7, %%ymm8, %%ymm9\n"
          "vperm2f128 $0, %%ymm7, %%ymm7, %%ymm10\n"
          "vperm2f128 $49, %%ymm9, %%ymm7, %%ymm11\n"
          "vaddps %%ymm10, %%ymm11, %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 8), "r"(buf + j + k + 16), "r"(buf + j + k + 24), "r"(buf + j + k + 32),
          "r"(buf + j + k + 40), "r"(buf + j + k + 48), "r"(buf + j + k + 56)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
            "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 4096; j += 512) {
    for (int k = 0; k < 64; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 64), "r"(buf + j + k + 128), "r"(buf + j + k + 192), "r"(buf + j + k + 256),
          "r"(buf + j + k + 320), "r"(buf + j + k + 384), "r"(buf + j + k + 448)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
            "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
  for (int j = 0; j < 4096; j += 4096) {
    for (int k = 0; k < 512; k += 8) {
      __asm__ volatile(
          "vmovups (%0), %%ymm0\n"
          "vmovups (%1), %%ymm1\n"
          "vmovups (%2), %%ymm2\n"
          "vmovups (%3), %%ymm3\n"
          "vmovups (%4), %%ymm4\n"
          "vmovups (%5), %%ymm5\n"
          "vmovups (%6), %%ymm6\n"
          "vmovups (%7), %%ymm7\n"
          "vaddps %%ymm1, %%ymm0, %%ymm8\n"
          "vsubps %%ymm1, %%ymm0, %%ymm9\n"
          "vaddps %%ymm3, %%ymm2, %%ymm10\n"
          "vsubps %%ymm3, %%ymm2, %%ymm11\n"
          "vaddps %%ymm5, %%ymm4, %%ymm12\n"
          "vsubps %%ymm5, %%ymm4, %%ymm13\n"
          "vaddps %%ymm7, %%ymm6, %%ymm14\n"
          "vsubps %%ymm7, %%ymm6, %%ymm15\n"
          "vaddps %%ymm10, %%ymm8, %%ymm0\n"
          "vsubps %%ymm10, %%ymm8, %%ymm2\n"
          "vaddps %%ymm11, %%ymm9, %%ymm1\n"
          "vsubps %%ymm11, %%ymm9, %%ymm3\n"
          "vaddps %%ymm14, %%ymm12, %%ymm4\n"
          "vsubps %%ymm14, %%ymm12, %%ymm6\n"
          "vaddps %%ymm15, %%ymm13, %%ymm5\n"
          "vsubps %%ymm15, %%ymm13, %%ymm7\n"
          "vaddps %%ymm4, %%ymm0, %%ymm8\n"
          "vsubps %%ymm4, %%ymm0, %%ymm12\n"
          "vaddps %%ymm5, %%ymm1, %%ymm9\n"
          "vsubps %%ymm5, %%ymm1, %%ymm13\n"
          "vaddps %%ymm6, %%ymm2, %%ymm10\n"
          "vsubps %%ymm6, %%ymm2, %%ymm14\n"
          "vaddps %%ymm7, %%ymm3, %%ymm11\n"
          "vsubps %%ymm7, %%ymm3, %%ymm15\n"
          "vmovups %%ymm8, (%0)\n"
          "vmovups %%ymm9, (%1)\n"
          "vmovups %%ymm10, (%2)\n"
          "vmovups %%ymm11, (%3)\n"
          "vmovups %%ymm12, (%4)\n"
          "vmovups %%ymm13, (%5)\n"
          "vmovups %%ymm14, (%6)\n"
          "vmovups %%ymm15, (%7)\n" ::"r"(buf + j + k + 0),
          "r"(buf + j + k + 512), "r"(buf + j + k + 1024), "r"(buf + j + k + 1536), "r"(buf + j + k + 2048),
          "r"(buf + j + k + 2560), "r"(buf + j + k + 3072), "r"(buf + j + k + 3584)
          : "%ymm0", "%ymm1", "%ymm2", "%ymm3", "%ymm4", "%ymm5", "%ymm6", "%ymm7", "%ymm8", "%ymm9", "%ymm10",
            "%ymm11", "%ymm12", "%ymm13", "%ymm14", "%ymm15", "memory");
    }
  }
}
